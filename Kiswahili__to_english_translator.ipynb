{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnghaa3rfzbmJbbLlCNgwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faithkym7/Special-topics---kiswahili-to-english-translator/blob/main/Kiswahili__to_english_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APFWYXXDuf3t",
        "outputId": "8498d2a5-32a8-4caa-9531-ff73ee5087bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1B51schTvzz3DYyQ03s-KECs-bSaEzhjR\n",
        "!gdown --id 1eBiew8h6O3RAeqwXd4xwrVLp9w--umNZ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR6TbEgWXAdY",
        "outputId": "9d5ff470-0a6f-46ac-9b20-a2e6e7c8f532"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (4.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.10.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1B51schTvzz3DYyQ03s-KECs-bSaEzhjR\n",
            "To: /content/english2.csv\n",
            "100% 13.7k/13.7k [00:00<00:00, 30.9MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eBiew8h6O3RAeqwXd4xwrVLp9w--umNZ\n",
            "To: /content/swahili2.csv\n",
            "100% 11.7k/11.7k [00:00<00:00, 29.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Read the data from two separate files\n",
        "\n",
        "kiswahili_data = pd.read_csv('swahili2.csv', encoding='ISO-8859-1', header=None, names=['Swahili sample sentences'])\n",
        "english_data = pd.read_csv('english2.csv', encoding='ISO-8859-1', header=None, names=['English sample sentence(s)'])\n",
        "\n",
        "\n",
        "# Remove any non-alphanumeric characters from the sentences\n",
        "kiswahili_data['Swahili sample sentences'] = kiswahili_data['Swahili sample sentences'].astype(str)\n",
        "kiswahili_data['Swahili sample sentences'] = kiswahili_data['Swahili sample sentences'].apply(lambda x: re.sub(r'\\W+', ' ', x).strip().lower())\n",
        "english_data['English sample sentence(s)'] = english_data['English sample sentence(s)'].astype(str)\n",
        "english_data['English sample sentence(s)'] = english_data['English sample sentence(s)'].apply(lambda x: re.sub(r'\\W+', ' ', x).strip().lower())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Combine the two datasets into one\n",
        "kiswahili_data = kiswahili_data.reset_index(drop=True)\n",
        "english_data = english_data.reset_index(drop=True)\n",
        "parallel_data = pd.concat([kiswahili_data, english_data], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data = parallel_data.sample(frac=0.8, random_state=42)\n",
        "val_data = parallel_data.drop(train_data.index)\n",
        "\n",
        "# Save the datasets to files\n",
        "train_data.to_csv('train_data.csv', index=False)\n",
        "val_data.to_csv('val_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "0RKtLL0QYXF7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train_data.csv')\n",
        "print(df.head(5))\n",
        "df = pd.read_csv('val_data.csv')\n",
        "print(df.head(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LckxnuzpblGU",
        "outputId": "348cf7a2-5e82-4dba-e6b7-64d5ab256f48"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            Swahili sample sentences  \\\n",
            "0  lengo langu ni kuelimisha watu kuwezesha mwana...   \n",
            "1                                namjua sijui unajua   \n",
            "2                 nimepotea mumepotea la hatujapotea   \n",
            "3  tuliamua kwenda mjomba wangu aliamua kuanza bi...   \n",
            "4                          anapendeza leo umependeza   \n",
            "\n",
            "                          English sample sentence(s)  \n",
            "0  my goal is to educate people empowering a woma...  \n",
            "1        i know this person i don t know do you know  \n",
            "2            i m lost are you lost no we re not lost  \n",
            "3  we decided to go my uncle decided to start a b...  \n",
            "4               she looks great today you look great  \n",
            "                            Swahili sample sentences  \\\n",
            "0  mwaka ijayo nitakuwa mwanafunzi nilikuwa na nj...   \n",
            "1  naomba ndizi na nyama tunaomba samaki naomba m...   \n",
            "2            tunaweza kuwasiliana kwa kiswahili sasa   \n",
            "3                        lengo langu ni kuwasiliiana   \n",
            "4  nitapata maji baadaye ninahitaji kupata chakul...   \n",
            "\n",
            "                          English sample sentence(s)  \n",
            "0  i will be a student next year i was hungry you...  \n",
            "1  i d like to order bananas and meat we d like t...  \n",
            "2                      we can communicate in swahili  \n",
            "3               my goal is to be able to communicate  \n",
            "4  i will get water later i need to get more food...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall keras tensorflow\n",
        "!pip install keras==2.4.3 tensorflow==2.4.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60c1f9ML1AzM",
        "outputId": "229e59c4-8da5-4a2a-dafd-afeab23adfbe"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 2.11.0\n",
            "Uninstalling keras-2.11.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.9/dist-packages/keras-2.11.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/keras/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled keras-2.11.0\n",
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow-2.11.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled tensorflow-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.4.3\n",
            "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.4.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "soBI8VDi16Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the training and validation data\n",
        "train_data = pd.read_csv('train_data.csv')\n",
        "val_data = pd.read_csv('val_data.csv')\n",
        "\n",
        "# Define a function to get the length of a string or return 0 for NaN values\n",
        "def get_length(x):\n",
        "    try:\n",
        "        return len(x)\n",
        "    except TypeError:\n",
        "        return 0\n",
        "\n",
        "# Define the maximum length of the input and output sequences\n",
        "max_encoder_seq_length = max(train_data['Swahili sample sentences'].apply(get_length))\n",
        "max_decoder_seq_length = max(train_data['English sample sentence(s)'].apply(get_length))\n",
        "\n",
        "\n",
        "\n",
        "# Define the number of encoder and decoder tokens\n",
        "encoder_num_tokens = 10000\n",
        "decoder_num_tokens = 10000\n",
        "\n",
        "train_data.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# Initialize the tokenizers for the encoder and decoder\n",
        "encoder_tokenizer = Tokenizer(num_words=encoder_num_tokens, filters='')\n",
        "encoder_tokenizer.fit_on_texts(train_data['Swahili sample sentences'])\n",
        "decoder_tokenizer = Tokenizer(num_words=decoder_num_tokens, filters='')\n",
        "decoder_tokenizer.fit_on_texts(train_data['English sample sentence(s)'])\n",
        "\n",
        "# Convert the text data to integer sequences\n",
        "encoder_input_data = encoder_tokenizer.texts_to_sequences(train_data['Swahili sample sentences'])\n",
        "decoder_input_data = decoder_tokenizer.texts_to_sequences(train_data['English sample sentence(s)'])\n",
        "decoder_target_data = decoder_tokenizer.texts_to_sequences(train_data['English sample sentence(s)'].apply(lambda x: ' '.join(x.split()[1:] + ['<eos>'])))\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "encoder_input_data = pad_sequences(encoder_input_data, maxlen=max_encoder_seq_length, padding='post')\n",
        "decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_decoder_seq_length, padding='post')\n",
        "decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_decoder_seq_length, padding='post')\n",
        "\n",
        "# Define the model architecture\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(encoder_num_tokens, 256)(encoder_inputs)\n",
        "encoder_lstm = LSTM(256, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(decoder_num_tokens, 256)(decoder_inputs)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(decoder_num_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "checkpoint = ModelCheckpoint('model_weights.h5', save_best_only=True)\n",
        "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmeeSZcfxSWo",
        "outputId": "c5a2ad26-de79-4283-a5c9-00befdb80098"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_22 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_23 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_15 (Embedding)       (None, None, 256)    2560000     ['input_22[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_16 (Embedding)       (None, None, 256)    2560000     ['input_23[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_15 (LSTM)                 [(None, 256),        525312      ['embedding_15[0][0]']           \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_16 (LSTM)                 [(None, None, 256),  525312      ['embedding_16[0][0]',           \n",
            "                                 (None, 256),                     'lstm_15[0][1]',                \n",
            "                                 (None, 256)]                     'lstm_15[0][2]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, None, 10000)  2570000     ['lstm_16[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,740,624\n",
            "Trainable params: 8,740,624\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 24s 5s/step - loss: 9.1398 - val_loss: 8.7902\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 14s 4s/step - loss: 7.9884 - val_loss: 6.5539\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 13s 3s/step - loss: 5.8146 - val_loss: 4.3403\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 14s 3s/step - loss: 3.4872 - val_loss: 2.0686\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 13s 3s/step - loss: 1.4986 - val_loss: 0.8784\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.7178 - val_loss: 0.6851\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5996 - val_loss: 0.6521\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5491 - val_loss: 0.6001\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.4974 - val_loss: 0.5688\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.4757 - val_loss: 0.5633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = load_model('model_weights.h5')\n",
        "encoder_tokenizer = Tokenizer(num_words=10000, filters='')\n",
        "encoder_tokenizer.fit_on_texts([' '.join(['<sos>', 'hello', '<eos>'])])  # Dummy input to initialize the tokenizer\n",
        "\n",
        "# Define the maximum length of the input and output sequences\n",
        "max_encoder_seq_length = 20\n",
        "max_decoder_seq_length = 20\n",
        "\n",
        "# Define a function to generate the English translation\n",
        "def translate_kiswahili_to_english(input_text):\n",
        "    # Convert the input text to a sequence of integer-encoded tokens\n",
        "    input_seq = encoder_tokenizer.texts_to_sequences([input_text])[0]\n",
        "    # Pad the input sequence to a fixed length\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_encoder_seq_length, padding='post')\n",
        "    # Initialize the decoder input sequence with the <sos> token\n",
        "    decoder_input_seq = np.zeros((1, max_decoder_seq_length))\n",
        "    decoder_input_seq[0, 0] = encoder_tokenizer.word_index['<sos>']\n",
        "    # Loop through the decoder until we reach the maximum sequence length or generate the <eos> token\n",
        "    for i in range(1, max_decoder_seq_length):\n",
        "        # Predict the next token in the sequence\n",
        "        output_tokens = model.predict([input_seq, decoder_input_seq]).argmax(axis=-1)\n",
        "        # If we generate the <eos> token, stop decoding\n",
        "        if output_tokens[0, i] == encoder_tokenizer.word_index['<eos>']:\n",
        "            break\n",
        "        # Add the predicted token to the decoder input sequence\n",
        "        decoder_input_seq[0, i] = output_tokens[0, i]\n",
        "    # Convert the output sequence of integer-encoded tokens to English text\n",
        "    output_text = ' '.join(encoder_tokenizer.index_word[i] for i in decoder_input_seq[0] if i > 0)\n",
        "    return output_text\n",
        "\n",
        "# Test the model with a sample input\n",
        "input_text = 'Habari yako'\n",
        "output_text = translate_kiswahili_to_english(input_text)\n",
        "print('Input text:', input_text)\n",
        "print('Translated text:', output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynCBRYa38R_9",
        "outputId": "3d59cd18-f099-4462-9fda-97673b1217ae"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe82375dca0> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x7fe82375dca0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe82375dca0> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x7fe82375dca0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1/1 [==============================] - 1s 712ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Input text: Habari yako\n",
            "Translated text: <sos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = load_model('model_weights.h5')\n",
        "encoder_tokenizer = Tokenizer(num_words=10000, filters='')\n",
        "encoder_tokenizer.fit_on_texts([' '.join(['<sos>', 'hello', '<eos>'])])  # Dummy input to initialize the tokenizer\n",
        "\n",
        "# Define the maximum length of the input and output sequences\n",
        "max_encoder_seq_length = 20\n",
        "max_decoder_seq_length = 20\n",
        "\n",
        "# Define a function to generate the English translation\n",
        "@tf.autograph.experimental.do_not_convert\n",
        "def predict_function(input_seq, decoder_input_seq):\n",
        "    return model.predict([input_seq, decoder_input_seq])\n",
        "\n",
        "def translate_kiswahili_to_english(input_text):\n",
        "    # Convert the input text to a sequence of integer-encoded tokens\n",
        "    input_seq = encoder_tokenizer.texts_to_sequences([input_text])[0]\n",
        "    # Pad the input sequence to a fixed length\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_encoder_seq_length, padding='post')\n",
        "    # Initialize the decoder input sequence with the <sos> token\n",
        "    decoder_input_seq = np.zeros((1, max_decoder_seq_length))\n",
        "    decoder_input_seq[0, 0] = encoder_tokenizer.word_index['<sos>']\n",
        "    # Loop through the decoder until we reach the maximum sequence length or generate the <eos> token\n",
        "    for i in range(1, max_decoder_seq_length):\n",
        "        # Predict the next token in the sequence\n",
        "        output_tokens = predict_function(input_seq, decoder_input_seq).argmax(axis=-1)\n",
        "        # If we generate the <eos> token, stop decoding\n",
        "        if output_tokens[0, i] == encoder_tokenizer.word_index['<eos>']:\n",
        "            break\n",
        "        # Add the predicted token to the decoder input sequence\n",
        "        decoder_input_seq[0, i] = output_tokens[0, i]\n",
        "    # Convert the output sequence of integer-encoded tokens to English text\n",
        "    output_text = ' '.join(encoder_tokenizer.index_word[i] for i in decoder_input_seq[0] if i > 0)\n",
        "    return output_text\n",
        "\n",
        "# Test the model with a sample input\n",
        "input_text = 'Nitakufahamisha'\n",
        "output_text = translate_kiswahili_to_english(input_text)\n",
        "print('Input text:', input_text)\n",
        "print('Translated text:', output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itJIBLLQ9IBT",
        "outputId": "807b973c-9290-4d4e-b360-8834e97aedde"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe823c30b80> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x7fe823c30b80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fe823c30b80> and will run it as-is.\n",
            "Cause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x7fe823c30b80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Input text: Nitakufahamisha\n",
            "Translated text: <sos>\n"
          ]
        }
      ]
    }
  ]
}